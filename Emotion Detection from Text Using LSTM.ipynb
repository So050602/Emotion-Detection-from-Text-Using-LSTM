{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9521251f-82bd-4e6e-98ad-5e855ea27f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing texts...\n",
      "Encoding labels...\n",
      "Splitting data...\n",
      "Tokenizing texts...\n",
      "Padding sequences...\n",
      "Creating model...\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumy\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10569/10569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1992s\u001b[0m 187ms/step - accuracy: 0.8303 - loss: 0.4055 - val_accuracy: 0.9377 - val_loss: 0.1005\n",
      "Epoch 2/10\n",
      "\u001b[1m10569/10569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1548s\u001b[0m 144ms/step - accuracy: 0.9394 - loss: 0.0979 - val_accuracy: 0.9412 - val_loss: 0.0920\n",
      "Epoch 3/10\n",
      "\u001b[1m10569/10569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2019s\u001b[0m 191ms/step - accuracy: 0.9413 - loss: 0.0907 - val_accuracy: 0.9418 - val_loss: 0.0897\n",
      "Epoch 4/10\n",
      "\u001b[1m10569/10569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2505s\u001b[0m 237ms/step - accuracy: 0.9424 - loss: 0.0867 - val_accuracy: 0.9417 - val_loss: 0.0895\n",
      "Epoch 5/10\n",
      "\u001b[1m10569/10569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1722s\u001b[0m 163ms/step - accuracy: 0.9419 - loss: 0.0859 - val_accuracy: 0.9418 - val_loss: 0.0933\n",
      "Epoch 6/10\n",
      "\u001b[1m10569/10569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1713s\u001b[0m 158ms/step - accuracy: 0.9441 - loss: 0.0834 - val_accuracy: 0.9413 - val_loss: 0.0928\n",
      "Epoch 7/10\n",
      "\u001b[1m10569/10569\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1701s\u001b[0m 161ms/step - accuracy: 0.9438 - loss: 0.0819 - val_accuracy: 0.9420 - val_loss: 0.0940\n",
      "\u001b[1m2643/2643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 62ms/step - accuracy: 0.9407 - loss: 0.0898\n",
      "\n",
      "Validation accuracy: 94.17%\n",
      "\n",
      "Testing predictions:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539ms/step\n",
      "\n",
      "Text: I am so happy today!\n",
      "Predicted emotion: joy\n",
      "Confidence: 98.77%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\n",
      "Text: I am anxious\n",
      "Predicted emotion: fear\n",
      "Confidence: 98.40%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\n",
      "Text: This is really frustratingThis made me feel very sad\n",
      "Predicted emotion: sad\n",
      "Confidence: 100.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\n",
      "Text: I'm really angry about what happened\n",
      "Predicted emotion: anger\n",
      "Confidence: 99.27%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\n",
      "Text: I'm feeling quite scared right now\n",
      "Predicted emotion: fear\n",
      "Confidence: 100.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\n",
      "Text: I finally got my dream job!\n",
      "Predicted emotion: fear\n",
      "Confidence: 54.17%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\n",
      "Text: I told you not to touch my things!\n",
      "Predicted emotion: anger\n",
      "Confidence: 77.09%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\n",
      "Text: I think someone is following me.\n",
      "Predicted emotion: anger\n",
      "Confidence: 69.92%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\n",
      "Text: I can’t believe I won the lottery!\n",
      "Predicted emotion: joy\n",
      "Confidence: 87.07%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
    "import re\n",
    "\n",
    "class EmotionClassifier:\n",
    "    def __init__(self, max_words=10000, max_len=100):\n",
    "        self.max_words = max_words\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.label_encoder = None\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def create_model(self, num_classes):\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            SpatialDropout1D(0.2),\n",
    "            LSTM(128, return_sequences=True),\n",
    "            LSTM(64),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def train(self, data_path, epochs=10, batch_size=32, validation_split=0.2):\n",
    "        try:\n",
    "            # Load data\n",
    "            print(\"Loading data...\")\n",
    "            df = pd.read_csv(data_path)\n",
    "            \n",
    "            # Preprocess texts\n",
    "            print(\"Preprocessing texts...\")\n",
    "            X = df['sentence'].apply(self.preprocess_text)\n",
    "            \n",
    "            # Encode labels\n",
    "            print(\"Encoding labels...\")\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            y = self.label_encoder.fit_transform(df['emotion'])\n",
    "            y = tf.keras.utils.to_categorical(y)\n",
    "            \n",
    "            # Split data\n",
    "            print(\"Splitting data...\")\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=validation_split, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Tokenize texts\n",
    "            print(\"Tokenizing texts...\")\n",
    "            self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "            self.tokenizer.fit_on_texts(X_train)\n",
    "            \n",
    "            # Convert texts to sequences\n",
    "            X_train_seq = self.tokenizer.texts_to_sequences(X_train)\n",
    "            X_val_seq = self.tokenizer.texts_to_sequences(X_val)\n",
    "            \n",
    "            # Pad sequences\n",
    "            print(\"Padding sequences...\")\n",
    "            X_train_pad = pad_sequences(X_train_seq, maxlen=self.max_len)\n",
    "            X_val_pad = pad_sequences(X_val_seq, maxlen=self.max_len)\n",
    "            \n",
    "            # Create and train model\n",
    "            print(\"Creating model...\")\n",
    "            self.model = self.create_model(len(self.label_encoder.classes_))\n",
    "            \n",
    "            # Add early stopping\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(\"Training model...\")\n",
    "            history = self.model.fit(\n",
    "                X_train_pad, y_train,\n",
    "                validation_data=(X_val_pad, y_val),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping]\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            val_loss, val_accuracy = self.model.evaluate(X_val_pad, y_val)\n",
    "            print(f\"\\nValidation accuracy: {val_accuracy*100:.2f}%\")\n",
    "            \n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during training: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, text):\n",
    "        try:\n",
    "            # Preprocess text\n",
    "            processed_text = self.preprocess_text(text)\n",
    "            \n",
    "            # Convert to sequence\n",
    "            sequence = self.tokenizer.texts_to_sequences([processed_text])\n",
    "            \n",
    "            # Pad sequence\n",
    "            padded = pad_sequences(sequence, maxlen=self.max_len)\n",
    "            \n",
    "            # Predict\n",
    "            prediction = self.model.predict(padded)\n",
    "            predicted_class = self.label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "            \n",
    "            # Get probability\n",
    "            probability = np.max(prediction) * 100\n",
    "            \n",
    "            return predicted_class[0], probability\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during prediction: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize classifier\n",
    "    classifier = EmotionClassifier(max_words=10000, max_len=100)\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        history = classifier.train(\n",
    "            data_path=r\"C:\\Users\\soumy\\Downloads\\Sentimental Analysis\\archive\\combined_emotion.csv\",\n",
    "            epochs=10,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # Example predictions\n",
    "        test_sentences = [\n",
    "            \"I am so happy today!\",\n",
    "            \"I am anxious\",\n",
    "            \"This is really frustrating\"\n",
    "            \"This made me feel very sad\",\n",
    "            \"I'm really angry about what happened\",\n",
    "            \"I'm feeling quite scared right now\",\n",
    "            \"I finally got my dream job!\",\n",
    "            \"I told you not to touch my things!\",\n",
    "            \"I think someone is following me.\",\n",
    "            \"I can’t believe I won the lottery!\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting predictions:\")\n",
    "        for sentence in test_sentences:\n",
    "            emotion, confidence = classifier.predict(sentence)\n",
    "            print(f\"\\nText: {sentence}\")\n",
    "            print(f\"Predicted emotion: {emotion}\")\n",
    "            print(f\"Confidence: {confidence:.2f}%\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d2b70-6200-4ce7-a988-336cad2c64fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
